A python cli application, built using the "click" framework. It takes prompts 
stored in the local filesystem and produces code files, stored along with the prompts.

The prompt files have ".prompt" suffix. For example, a prompt to produce example.py 
will be stored in the example.py.prompt file.

Prompt files may contain @raw(filename) and @prompt(filename) directives. Example:

```
  @raw(path/to/some/file.py)
  Some text @prompt(path/to/another/file.py.prompt) more text.
```
The directives may be inside the text. Use regular expressions to find them.
The application shall replace these directives with the contents of the specified file.
If the file included using @prompt directive itself contains @prompt directives, they should be expanded as well.
The user may avoid exansion by passing the --skip-directives parameter.

To produce code from the prompt please use llm_invoke function from the .models module in the same folder.
The function takes three params:
- provider, which can be either 'bedrock' or 'anthropic', the default being 'bedrock'. Let the user specify the needed
provider using --provider parameter.
- model, the default being 'anthropic.claude-3-5-sonnet-20240620-v1:0'. Let the user specify the needed model
using the --model parameter.
- messages, an array of chat messages to pass to the model
The function returns LLM's answer.

Use these system instructions for the AI:
<instructions>Output just the file content, nothing else. Do not print any 
introductory text before the code. Do not enclose the code in triple quotes. 
Include a comment in the code
saying that this file was produced by AI from a prompt and shouldn't 
be edited directly, unless generating json or other file formats
that don't support comments.</instructions>

Also provide a one-shot example, such as 
<user>Hello world in python</user>
<assistant>print('Hello world')</assistant>

Name the entry point function as 'main'.

Please avoid these errors and warnings:
ValueError: System message must be at beginning of message list.
